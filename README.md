# Optimisation-Sans-Contrainte

# TP : Algorithmes de Descente de Gradient & Optimisation Sans Contrainte

**Universit√© de Yaound√© 1** **Master 1 Data Science** **Cours d'Optimisation**

---

##  Description du Projet

Ce d√©p√¥t contient l'ensemble des exp√©rimentations pratiques men√©es dans le cadre du cours d'optimisation sans contrainte. L'objectif est d'impl√©menter, de visualiser et d'analyser le comportement des algorithmes de gradient (Premier Ordre) sur des topologies de fonctions vari√©es, allant du cas convexe simple aux surfaces pathologiques non-convexes.

L'√©tude se concentre sur la comparaison de deux strat√©gies de pas :
1.  **La M√©thode de Plus Profonde Descente (Steepest Descent) :** Utilisation d'un pas optimal calcul√© analytiquement (recherche lin√©aire exacte).
2.  **La Descente de Gradient √† Pas Fixe :** Utilisation d'un taux d'apprentissage constant ($s$).

##  Contenu du Repository

Les notebooks Jupyter pr√©sentent l'analyse math√©matique et la simulation num√©rique des fonctions suivantes :

### 1. Fonction Quadratique (Mal Conditionn√©e)
$$f(x, y) = \frac{1}{2}x^2 + \frac{7}{2}y^2$$
* **Objectif :** Analyser l'impact du conditionnement de la matrice Hessienne.
* **Observation Cl√© :** Mise en √©vidence du ph√©nom√®ne de **zigzag** (oscillations orthogonales) caract√©ristique de la m√©thode de plus profonde descente dans les vall√©es √©troites, et sensibilit√© critique du choix du scalaire $s$ pour le pas fixe.

### 2. Fonction de Rosenbrock ("The Banana Function")
$$f(x, y) = (1-x)^2 + 100(y - x^2)^2$$
* **Objectif :** Tester la robustesse des algorithmes sur une fonction non-convexe avec une vall√©e parabolique √©troite.
* **Observation Cl√© :** Les m√©thodes de premier ordre convergent tr√®s lentement le long de la courbe de la vall√©e ($y=x^2$), illustrant la n√©cessit√© de m√©thodes adaptatives (type Adam ou Newton) pour les probl√®mes complexes.

### 3. Fonction Point Selle (Ind√©finie)
$$f(x, y) = x^2 - y^2$$
* **Objectif :** √âtudier le comportement au voisinage d'un point stationnaire qui n'est pas un extremum (Point Selle en 0,0).
* **Observation Cl√© :** D√©monstration de la **divergence** de l'algorithme le long de la direction de courbure n√©gative (axe $y$), soulignant le danger des points selles dans l'optimisation des r√©seaux de neurones.

### 4. Fonction de Himmelblau (Multi-modale)
$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$
* **Objectif :** Optimisation sur une surface poss√©dant **4 minima globaux** distincts.
* **Observation Cl√© :** Analyse de la d√©pendance au point initial ($x_0$). L'algorithme converge vers l'un des 4 minima en fonction du bassin d'attraction dans lequel il est initialis√©.

##  Installation et Usage

Pour reproduire les r√©sultats, clonez ce d√©p√¥t et lancez les notebooks via Jupyter ou Google Colab.
# TP : Algorithmes de Descente de Gradient & Optimisation Sans Contrainte

**Universit√© de Yaound√© 1** **Master 1 Data Science** **Cours d'Optimisation**

---

## üìã Description du Projet

Ce d√©p√¥t contient l'ensemble des exp√©rimentations pratiques men√©es dans le cadre du cours d'optimisation sans contrainte. L'objectif est d'impl√©menter, de visualiser et d'analyser le comportement des algorithmes de gradient (Premier Ordre) sur des topologies de fonctions vari√©es, allant du cas convexe simple aux surfaces pathologiques non-convexes.

L'√©tude se concentre sur la comparaison de deux strat√©gies de pas :
1.  **La M√©thode de Plus Profonde Descente (Steepest Descent) :** Utilisation d'un pas optimal calcul√© analytiquement (recherche lin√©aire exacte).
2.  **La Descente de Gradient √† Pas Fixe :** Utilisation d'un taux d'apprentissage constant ($s$).

## üõ†Ô∏è Contenu du Repository

Les notebooks Jupyter pr√©sentent l'analyse math√©matique et la simulation num√©rique des fonctions suivantes :

### 1. Fonction Quadratique (Mal Conditionn√©e)
$$f(x, y) = \frac{1}{2}x^2 + \frac{7}{2}y^2$$
* **Objectif :** Analyser l'impact du conditionnement de la matrice Hessienne.
* **Observation Cl√© :** Mise en √©vidence du ph√©nom√®ne de **zigzag** (oscillations orthogonales) caract√©ristique de la m√©thode de plus profonde descente dans les vall√©es √©troites, et sensibilit√© critique du choix du scalaire $s$ pour le pas fixe.

### 2. Fonction de Rosenbrock ("The Banana Function")
$$f(x, y) = (1-x)^2 + 100(y - x^2)^2$$
* **Objectif :** Tester la robustesse des algorithmes sur une fonction non-convexe avec une vall√©e parabolique √©troite.
* **Observation Cl√© :** Les m√©thodes de premier ordre convergent tr√®s lentement le long de la courbe de la vall√©e ($y=x^2$), illustrant la n√©cessit√© de m√©thodes adaptatives (type Adam ou Newton) pour les probl√®mes complexes.

### 3. Fonction Point Selle (Ind√©finie)
$$f(x, y) = x^2 - y^2$$
* **Objectif :** √âtudier le comportement au voisinage d'un point stationnaire qui n'est pas un extremum (Point Selle en 0,0).
* **Observation Cl√© :** D√©monstration de la **divergence** de l'algorithme le long de la direction de courbure n√©gative (axe $y$), soulignant le danger des points selles dans l'optimisation des r√©seaux de neurones.

### 4. Fonction de Himmelblau (Multi-modale)
$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$
* **Objectif :** Optimisation sur une surface poss√©dant **4 minima globaux** distincts.
* **Observation Cl√© :** Analyse de la d√©pendance au point initial ($x_0$). L'algorithme converge vers l'un des 4 minima en fonction du bassin d'attraction dans lequel il est initialis√©.

## üöÄ Installation et Usage

Pour reproduire les r√©sultats, clonez ce d√©p√¥t et lancez les notebooks via Jupyter ou Google Colab.

```bash
git clone [https://github.com/Data-Geniuses-SAAJ/Optimisation-Sans-Contrainte.git](https://github.com/Data-Geniuses-SAAJ/Optimisation-Sans-Contrainte.git)
cd Optimisation-Sans-Contrainte
jupyter notebook
